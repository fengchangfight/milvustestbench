{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Image Search with Milvus\n",
    "\n",
    "This notebook demonstrates multimodal image search using Visualized BGE embeddings. It combines image and text queries to find similar images, then uses GPT-4o to rank and explain the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all required libraries for multimodal image search and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'visual_bge'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Add the visual_bge module to Python path\u001b[39;00m\n\u001b[32m     14\u001b[39m sys.path.append(os.path.join(os.path.dirname(\u001b[33m'\u001b[39m\u001b[33m/home/fengchang/test/milvustestbench\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFlagEmbedding\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mresearch\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvisual_bge\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mvisual_bge\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Visualized_BGE\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'visual_bge'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pymilvus import MilvusClient\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Add the visual_bge module to Python path\n",
    "sys.path.append(os.path.join(os.path.dirname('/home/fengchang/test/milvustestbench/multimodal.py'), 'data', 'FlagEmbedding', 'research', 'visual_bge'))\n",
    "from visual_bge.modeling import Visualized_BGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Class\n",
    "\n",
    "Define the Encoder class that handles multimodal embedding generation using Visualized BGE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, model_name: str, model_path: str):\n",
    "        self.model = Visualized_BGE(model_name_bge=model_name, model_weight=model_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode_query(self, image_path: str, text: str) -> list[float]:\n",
    "        with torch.no_grad():\n",
    "            query_emb = self.model.encode(image=image_path, text=text)\n",
    "        return query_emb.tolist()[0]\n",
    "\n",
    "    def encode_image(self, image_path: str) -> list[float]:\n",
    "        with torch.no_grad():\n",
    "            query_emb = self.model.encode(image=image_path)\n",
    "        return query_emb.tolist()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Encoder\n",
    "\n",
    "Load the Visualized BGE model for multimodal embedding generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder with BGE model\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_path = \"data/Visualized_base_en_v1.5.pth\"\n",
    "encoder = Encoder(model_name, model_path)\n",
    "print(\"Encoder initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Image Embeddings\n",
    "\n",
    "Process all images in the dataset and generate their embeddings for vector search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the image dataset\n",
    "data_dir = \"data/images_folder\"\n",
    "image_list = glob(os.path.join(data_dir, \"images\", \"*.jpg\"))\n",
    "image_dict = {}\n",
    "\n",
    "print(f\"Found {len(image_list)} images to process...\")\n",
    "\n",
    "for image_path in tqdm(image_list, desc=\"Generating image embeddings: \"):\n",
    "    try:\n",
    "        image_dict[image_path] = encoder.encode_image(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate embedding for {image_path}. Skipped.\")\n",
    "        continue\n",
    "\n",
    "print(f\"Number of encoded images: {len(image_dict)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Milvus Database\n",
    "\n",
    "Create a Milvus collection and insert the image embeddings for vector search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding dimension\n",
    "dim = len(list(image_dict.values())[0])\n",
    "collection_name = \"multimodal_rag_demo\"\n",
    "\n",
    "# Connect to Milvus client\n",
    "milvus_client = MilvusClient(\n",
    "    uri=\"http://localhost:19530\",\n",
    "    token=\"root:Milvus\"\n",
    ")\n",
    "\n",
    "# Drop existing collection if it exists\n",
    "if milvus_client.has_collection(collection_name=collection_name):\n",
    "    milvus_client.drop_collection(collection_name=collection_name)\n",
    "    print(f\"Dropped existing collection: {collection_name}\")\n",
    "\n",
    "# Create Milvus Collection\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    auto_id=True,\n",
    "    dimension=dim,\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "print(f\"Created collection: {collection_name} with dimension: {dim}\")\n",
    "\n",
    "# Insert data into collection\n",
    "milvus_client.insert(\n",
    "    collection_name=collection_name,\n",
    "    data=[{\"image_path\": k, \"vector\": v} for k, v in image_dict.items()],\n",
    ")\n",
    "print(\"Data inserted successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Multimodal Search\n",
    "\n",
    "Search for similar images using both image and text query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query image and text\n",
    "query_image = os.path.join(data_dir, \"leopard.jpg\")\n",
    "query_text = \"phone case with this image theme\"\n",
    "\n",
    "print(f\"Query image: {query_image}\")\n",
    "print(f\"Query text: {query_text}\")\n",
    "\n",
    "# Generate query embedding given image and text instructions\n",
    "query_vec = encoder.encode_query(image_path=query_image, text=query_text)\n",
    "\n",
    "# Search for similar images\n",
    "search_results = milvus_client.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[query_vec],\n",
    "    output_fields=[\"image_path\"],\n",
    "    limit=9,  # Max number of search results to return\n",
    "    search_params={\"metric_type\": \"COSINE\", \"params\": {}},\n",
    ")[0]\n",
    "\n",
    "retrieved_images = [hit.get(\"entity\").get(\"image_path\") for hit in search_results]\n",
    "print(f\"Found {len(retrieved_images)} similar images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Function\n",
    "\n",
    "Create a panoramic view function to display query image and search results in a grid layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image display parameters\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "row_count = 3\n",
    "\n",
    "def create_panoramic_view(query_image_path: str, retrieved_images: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a panoramic view image from a query image and retrieved results.\n",
    "    \n",
    "    Layout:\n",
    "    - Left column: Query image (with blue border and \"query\" label)\n",
    "    - Right 2x3 grid: Retrieved images (with red index numbers)\n",
    "    \n",
    "    Args:\n",
    "        query_image_path: Path to the query image\n",
    "        retrieved_images: List of paths to retrieved images\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The panoramic view image\n",
    "    \"\"\"\n",
    "    panoramic_width = img_width * row_count\n",
    "    panoramic_height = img_height * row_count\n",
    "    panoramic_image = np.full(\n",
    "        (panoramic_height, panoramic_width, 3), 255, dtype=np.uint8\n",
    "    )\n",
    "\n",
    "    # Create and resize the query image with a blue border\n",
    "    query_image_null = np.full((panoramic_height, img_width, 3), 255, dtype=np.uint8)\n",
    "    query_image = Image.open(query_image_path).convert(\"RGB\")\n",
    "    query_array = np.array(query_image)[:, :, ::-1]\n",
    "    resized_image = cv2.resize(query_array, (img_width, img_height))\n",
    "\n",
    "    border_size = 10\n",
    "    blue = (255, 0, 0)  # blue color in BGR\n",
    "    bordered_query_image = cv2.copyMakeBorder(\n",
    "        resized_image,\n",
    "        border_size,\n",
    "        border_size,\n",
    "        border_size,\n",
    "        border_size,\n",
    "        cv2.BORDER_CONSTANT,\n",
    "        value=blue,\n",
    "    )\n",
    "\n",
    "    query_image_null[img_height * 2 : img_height * 3, 0:img_width] = cv2.resize(\n",
    "        bordered_query_image, (img_width, img_height)\n",
    "    )\n",
    "\n",
    "    # Add text \"query\" below the query image\n",
    "    text = \"query\"\n",
    "    font_scale = 1\n",
    "    font_thickness = 2\n",
    "    text_org = (10, img_height * 3 + 30)\n",
    "    cv2.putText(\n",
    "        query_image_null,\n",
    "        text,\n",
    "        text_org,\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        font_scale,\n",
    "        blue,\n",
    "        font_thickness,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "    # Combine the rest of the images into the panoramic view\n",
    "    retrieved_imgs = [\n",
    "        np.array(Image.open(img).convert(\"RGB\"))[:, :, ::-1] for img in retrieved_images\n",
    "    ]\n",
    "    for i, image in enumerate(retrieved_imgs):\n",
    "        image = cv2.resize(image, (img_width - 4, img_height - 4))\n",
    "        row = i // row_count\n",
    "        col = i % row_count\n",
    "        start_row = row * img_height\n",
    "        start_col = col * img_width\n",
    "\n",
    "        border_size = 2\n",
    "        bordered_image = cv2.copyMakeBorder(\n",
    "            image,\n",
    "            border_size,\n",
    "            border_size,\n",
    "            border_size,\n",
    "            border_size,\n",
    "            cv2.BORDER_CONSTANT,\n",
    "            value=(0, 0, 0),\n",
    "        )\n",
    "        panoramic_image[\n",
    "            start_row : start_row + img_height, start_col : start_col + img_width\n",
    "        ] = bordered_image\n",
    "\n",
    "        # Add red index numbers to each image\n",
    "        text = str(i)\n",
    "        org = (start_col + 50, start_row + 30)\n",
    "        (font_width, font_height), baseline = cv2.getTextSize(\n",
    "            text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2\n",
    "        )\n",
    "\n",
    "        top_left = (org[0] - 48, start_row + 2)\n",
    "        bottom_right = (org[0] - 48 + font_width + 5, org[1] + baseline + 5)\n",
    "\n",
    "        cv2.rectangle(\n",
    "            panoramic_image, top_left, bottom_right, (255, 255, 255), cv2.FILLED\n",
    "        )\n",
    "        cv2.putText(\n",
    "            panoramic_image,\n",
    "            text,\n",
    "            (start_col + 10, start_row + 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 0, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    # Combine the query image with the panoramic view\n",
    "    panoramic_image = np.hstack([query_image_null, panoramic_image])\n",
    "    return panoramic_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Display Results\n",
    "\n",
    "Generate the panoramic view and display the search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create panoramic view\n",
    "combined_image_path = os.path.join(data_dir, \"combined_image.jpg\")\n",
    "panoramic_image = create_panoramic_view(query_image, retrieved_images)\n",
    "cv2.imwrite(combined_image_path, panoramic_image)\n",
    "\n",
    "# Display the combined image\n",
    "combined_image = Image.open(combined_image_path)\n",
    "show_combined_image = combined_image.resize((300, 300))\n",
    "display(show_combined_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o Ranking and Explanation\n",
    "\n",
    "Use GPT-4o to analyze the search results and provide intelligent ranking with explanations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai_api_key = \"sk-***\"  # Change to your OpenAI API Key\n",
    "\n",
    "def generate_ranking_explanation(\n",
    "    combined_image_path: str, caption: str, infos: dict = None\n",
    ") -> tuple[list[int], str]:\n",
    "    \"\"\"\n",
    "    Use GPT-4o to analyze search results and provide intelligent ranking.\n",
    "    \n",
    "    Args:\n",
    "        combined_image_path: Path to the combined panoramic image\n",
    "        caption: User's text query/instruction\n",
    "        infos: Optional additional information about products\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ranked_indices, explanation)\n",
    "    \"\"\"\n",
    "    with open(combined_image_path, \"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    information = (\n",
    "        \"You are responsible for ranking results for a Composed Image Retrieval. \"\n",
    "        \"The user retrieves an image with an 'instruction' indicating their retrieval intent. \"\n",
    "        \"For example, if the user queries a red car with the instruction 'change this car to blue,' a similar type of car in blue would be ranked higher in the results. \"\n",
    "        \"Now you would receive instruction and query image with blue border. Every item has its red index number in its top left. Do not misunderstand it. \"\n",
    "        f\"User instruction: {caption} \\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Add additional information for each image\n",
    "    if infos:\n",
    "        for i, info in enumerate(infos[\"product\"]):\n",
    "            information += f\"{i}. {info}\\n\"\n",
    "\n",
    "    information += (\n",
    "        \"Provide a new ranked list of indices from most suitable to least suitable, followed by an explanation for the top 1 most suitable item only. \"\n",
    "        \"The format of the response has to be 'Ranked list: []' with the indices in brackets as integers, followed by 'Reasons:' plus the explanation why this most fit user's query intent.\"\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": information},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload\n",
    "    )\n",
    "    result = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Parse the ranked indices from the response\n",
    "    start_idx = result.find(\"[\")\n",
    "    end_idx = result.find(\"]\")\n",
    "    ranked_indices_str = result[start_idx + 1 : end_idx].split(\",\")\n",
    "    ranked_indices = [int(index.strip()) for index in ranked_indices_str]\n",
    "\n",
    "    # Extract explanation\n",
    "    explanation = result[end_idx + 1 :].strip()\n",
    "\n",
    "    return ranked_indices, explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate AI Ranking and Display Best Result\n",
    "\n",
    "Use GPT-4o to intelligently rank the search results and display the best match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate AI ranking and explanation\n",
    "ranked_indices, explanation = generate_ranking_explanation(\n",
    "    combined_image_path, query_text\n",
    ")\n",
    "\n",
    "print(\"AI Ranking Explanation:\")\n",
    "print(explanation)\n",
    "\n",
    "# Display the best result\n",
    "best_index = ranked_indices[0]\n",
    "best_img = Image.open(retrieved_images[best_index])\n",
    "best_img = best_img.resize((150, 150))\n",
    "print(f\"\\nBest match (index {best_index}):\")\n",
    "display(best_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
